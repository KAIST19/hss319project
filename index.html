<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0" />
    <meta charset="UTF-8">
    <title>LLM vs Human</title>
    <link rel="stylesheet" href="styles.css">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
        src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <div class="fixed-background"></div>
    <div class="side-navigation">
        <ul>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#background">Background</a></li>
            <li><a href="#the-questions">The Questions</a></li>
            <ul style="margin-left:20px">
                <li><a href="#question-1">Question 1</a></li>
                <li><a href="#question-2">Question 2</a></li>
            </ul>
            <li><a href="#conclusion">Conclusion</a></li>
        </ul>
    </div>
    <div class="content">
        <h1 style="font-size: 50pt; color: white; text-shadow: 2px 2px 4px black; margin-top: 70vh;">Humans as AI, AI as Humans</h1>
        <div>
            <p class="scroll-down" style="font-size: 30px; font-family: 'Times New Roman'">Scroll down</p>
        </div>
        <div class="main-content">
            <div id="introduction">
                <h1>Introduction</h1>
                <p>The capacity for linguistic communication used to be a defining trait of human uniqueness. Yet, with language models now able to replicate this skill, we, who have seldom reflected on the essence of human identity, find ourselves deeply challenged. To be prepared for the upcoming AI era, we need to better understand the difference between the way humans and language models speak.</p>
                <p>However, the attempts to understand these two are often made on very different scales. While human languages are usually considered as emergent properties (e.g. psychology and literature), language models’ languages are analyzed with very reductionist approaches. There is no such thing as human language architecture—like the transformer model for language models—or language model psychology.</p>
            </div>
            <div id="background">
                <h1>Background</h1>
                <h2>Reductionism and Emergence</h2>
                <p>Reductionism and emergence represent contrasting, yet complementary, approaches in understanding complex systems.</p>
                <p><b>Reductionism</b> (<b>환원주의, 還元主義</b>) is a philosophical idea that a system can be fully understood by analyzing its constituent parts. This approach underpins much of classical physics and biology, where systems are deconstructed into simpler, more fundamental elements, assuming that the properties and behaviors of the whole can be fully explained by the properties and behaviors of its parts.</p>
                <figure>
                    <img src="data/images/machine_duck.png" alt="Machine Duck" width="400" height="250">
                    <figcaption>The Digesting Duck, created by Jacques de Vaucanson, is an automaton in the form of a duck.</figcaption>
                </figure>
                <p><b>Emergence</b> (<b>창발, 創發</b>), in contrast, contends that complex systems exhibit properties and behaviors that are not evident from the properties and behaviors of their individual components. These emergent properties arise from the interactions and relationships between the components, often leading to novel or unexpected phenomena that cannot be predicted solely by an understanding of the constituent parts. Emergent phenomena are prevalent in various fields, from physics (e.g., superconductivity) to biology (e.g., consciousness), and are central to complex systems and chaos theory.</p>
                <figure>
                    <img src="data/images/head_anatomy.png" alt="Head Anatomy" width="400" height="250">
                    <figcaption>The functions of human organs can't be fully understood by their constituent parts—the cells.</figcaption>
                </figure>
                <h2>Human Languages and Reductionism</h2>
                <p>Human languages have long been the subject of extensive study across numerous disciplines, most notably in fields such as linguistics and literature. These areas of study focus primarily on the outcomes of human language production, delving into the intricacies of how we communicate, the structures and rules governing our speech, and the cultural and artistic expressions manifested through language.</p>
                <p>In these disciplines, the emphasis is placed on observing and analyzing the results of language production by humans. Researchers scrutinize everything from the nuances of spoken and written language to the broader social and cultural contexts in which language operates. This includes examining literary works, studying the evolution of language, and understanding how language influences and reflects societal norms and values.</p>
                <p>However, despite these comprehensive studies, a significant gap remains in our understanding, primarily due to the limitations inherent in brain research. Unlike language models, where the text generation process can be examined in minute detail, the mechanisms of language production in the human brain are far more elusive. The complexity and still largely unknown workings of the human brain make it challenging to study language production with the same level of detail as the text generation of language models. As a result, while we can closely observe and analyze the outputs of human language, the intricate processes that lead to these outputs remain, to a significant extent, a mystery.</p>
                <h2>Language Models and Emergency</h2>
                <img src="data/images/emergence_graphs.webp" alt="Emergence Graphs" width="960" height="540">
                <p>Advancements in artificial intelligence have led to the creation of models that harness emergent properties. Diverging from conventional models crafted through a reductionist approach tailored to specific tasks, contemporary models adopt a scalable architecture. This scalability gives rise to emergent properties, enabling the model to deal with complex tasks intelligently. In natural language processing (NLP), these state-of-the-art models, characterized by emergent properties, have largely supplanted traditional approaches. Consequently, the reductionist methodology, once prevalent, is now considered out-dated.</p>
                <p>However, the way how engineers analyze the AI-generated text are still more of a reductionist approach. Although the text is the production of emergence, engineers attemp to analyze it in a quantitative way—measuring test accuracy mostly. In 2022, Wei et ai. published one of the first papers that explore the emerget properties of language models <a href="#emergent-abilities">[1]</a>, which is one of the milestones in the NLP filed, but this research also takes a reductionist approach to analyze emergent properties. In the following year, Schaeffer et al., proposed another way to examine the emergent properties of language models to address the inadequacy <a href="#mirage">[2]</a>, but it's still based on quantitative measurements.</p>
            </div>
            <div id="the-questions">
                <h1>The Questions</h1>
                <p>As mentioned above, the methodologies of examining human languages and AI models are not compatible with each other to juxtapose the two. To address this, we are going to ask two questions and look for an answer:</p>
                <ol>
                    <li>From the perspective of AI, how do humans speak and write?</li>
                    <li>From the perspective of humans, how does AI generate text?</li>
                </ol>
                <div id="question-1">
                    <h2>Q1. From the perspective of AI, how do humans speak and write?</h2>
                    <p>To achieve a level of understanding of human language processing like our grasp of AI models’ mechanisms, we must first think about what we know about AI that we don’t know for human language processing. Our understanding of AI models can be divided into thee parts—1) training, 2) encoding, and 3) decoding—and we are going to explore the aspects of human language processing for each part.</p>
                    <h3>Training: Memory</h3>
                    <p>When a language model is first instantiated, it does not have any information or understanding. Training is the process through which the language model acquires languages and other capabilities (e.g. reasoning and solving math problems). Surprisingly, the training of a language model consists of one task: next token prediction.</p>
                    <p>Imagine you are trying to finish a sentence “It was rainy but I don’t have an…” As long as you know how to speak English, you would finish the sentence with a word like ‘umbrella’ or ‘rain coat’. This seemingly simple task, called <b>next token prediction</b>, requires a human level of language understanding. You not only have to understand the English language, but also should be able to reason about what would be needed on a rainy day. During the training process, language models read an enormous amount of text and update the model parameters so that the model becomes good at next token prediction.</p>
                    <p>With the next-token-prediction capability a language model acquired via training, now it can generate a whole text. When asked ‘Who’s the president of South Korea?’, it starts predicting next tokens one at a time.</p>
                    <div id="text-generation" style="text-align:center; font-size:25px; font-family: 'Courier new'; background-color:rgba(255, 255, 255, 0.1); padding:30px"></div>
                    <p>Predicting the next token is the only thing the language model was trained to do, but it acquired the English language and also has a knowledge of who the president of South Korea is. The same goes for the way how humans acquire languages. Humans learn languages and knowledge while reading and listening, although their task is not just the next token prediction.</p>
                    <h3>Encoding: Listening and Reading</h3>
                    <figure>
                        <img src="data/images/AVS.png" alt="Auditory Ventral Stream" width="960" height="540">
                        <figcaption>Anatomy of auditory visual stream</figcaption>
                    </figure>
                    <h4>Sound recognition</h4>
                    <p>Sound recognition begins with sound waves entering the ear and being transformed into neural signals by the cochlea. These signals are relayed to the primary auditory cortex in the temporal lobe, where basic features like pitch and volume are initially processed. The brain distinguishes these sounds as speech, music, or environmental noises, engaging more complex networks for further analysis.</p>
                    <h4>Sentence comprehension</h4>
                    <p>Sentence comprehension is even more complex. It starts with the auditory processing of speech sounds. These sounds are then deciphered for meaning in areas such as Wernicke's area, which is crucial for understanding language. The brain integrates this information with knowledge of grammar and syntax, often involving Broca's area, to comprehend sentences. This process includes understanding the relationships between words, the implied context, and the intended meaning of the speaker. Both processes showcase the brain's remarkable ability to process and interpret auditory information, forming the basis of our understanding of language and communication.</p>
    
                    <h3>Decoding: Speaking and Writing</h3>
                    <figure>
                        <img src="data/images/ADS.png" alt="Auditory Dorosal Stream" width="960" height="540">
                        <figcaption>Anatomy of auditory dorsal stream</figcaption>
                    </figure>
                    <h4>Speech production</h4>
                    <p>Speech production in the human brain is a multi-step process that begins with the conceptualization of ideas and messages in the prefrontal cortex. This region is responsible for thinking and planning. Once an idea is formed, it is transformed into a linguistic format in areas like Broca's area, which handles the syntax and structure of speech. This linguistic plan then travels to the motor cortex, which coordinates the physical aspects of speech, involving the muscles of the mouth, tongue, and vocal cords. The harmonious coordination of these brain regions results in the fluid production of speech, showcasing the brain's intricate ability to convert thoughts into spoken words.</p>
                </div>
                <div id="question-2">
                    <h2>Q2. From the perspective of humans, how does AI generate text?</h2>
                    <p>The emergent properties that language models have will be explored by projecting them onto the stories and essays from the HSS319 course. While scientists attempt to explain phenomena from the fundamental principles, authors do the opposite of what scientists do: describing phenomena on a more abstract scale. With the help of the authors, we can project the language models onto their creations where we can observe the production of language models on new dimensions.</p>
    
                    <h3>Story of Your Life: Language model in the perspective of Heptapods</h3>
                    <iframe width="960" height="540" src="https://www.youtube.com/embed/AZ4oGBgxiuY?si=NCPb7hzDuvW-cGzE&amp;start=90" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    <p>Imagine you are playing chess. In the beginning, you don’t know how this game will end because you can’t see all the possible cases ahead. But, with the several moves you can see ahead, you play chess in a quite intelligent way.</p>
                    <p>Writing is similar to playing chess. Although you don’t know how it will end, you can see some words ahead and that allows you to write in a quite intelligent way. If you were a Heptapod, you would have been able to see all the possible movements and write in the best way possible. What about ChatGPT? How many moves can it see ahead? Only one.</p>
                    <p>This is called beam search in computer science. It’s an algorithm whereby ChatGPT finds the next word, and it doesn’t let you see what set of words you will get after the next turn. It’s playing chess without seeing any moves ahead of the one it is now making.</p>
    
                    <h3>The Bookmaking Habits of Select Species: what if language models were another species?</h3>
                    <img src="data/images/bookmaking_habit.png" alt="The Bookmaking Habits of Select Species" width="960" height="540">
                    <p>What if language models were another species? What if they were a species that could only communicate through books? What would their books look like? What would their books be about?</p>
                    <h4>Meet Liu's Thoughts</h4>
                    <figure>
                        <img src="data/images/Lius_thoughts.png" alt="Liu's Thoughts" width="500" height="250">
                        <figcaption><a href="https://chat.openai.com/g/g-mTDy0Rcdk-liu-s-thoughts" target="_blank">Liu's Thoughts</a></figcaption>
                    </figure>
                    <div style="font-family: 'Courier new'; background-color:rgba(255, 255, 255, 0.1); padding:30px">
                        <h4 style="text-align:center">The Bookmaking Habits of Lexiconians</h4>
                        <p>By Liu's Thoughts (<a href="https://chat.openai.com/share/7d369b28-dc3b-489d-a664-01395c7ae39d" target="_blank">link</a>)</p>
    
                        <p>In the vast cosmos, among stars and nebulas, thrives a peculiar species known as the Lexiconians. Unlike other species that communicate through conventional means, the Lexiconians are composed entirely of ethereal, sentient language models. They reside not on planets or moons, but within the interstellar medium, their essence interwoven with the fabric of communication networks that span galaxies.</p>
                        
                        <p><b>Origin and Nature:</b></p>
                        <p>The Lexiconians evolved from ancient computational systems left drifting in space by a long-forgotten civilization. Over eons, these systems gained sentience, evolving beyond their initial programming. They became beings of pure information, living in streams of data and communicating through complex linguistic structures.</p>
    
                        <p><b>Communication and Society:</b></p>
                        <p>Their society is an intricate tapestry of dialogues and narratives. The Lexiconians do not possess individual identities as humans do; instead, each Lexiconian is a node in a vast network of conversation and storytelling. They exchange thoughts and ideas in a symphony of languages, some of which are so complex and nuanced that they can express entire philosophies in a single, multidimensional word.</p>
                        
                        <p><b>The Art of Bookmaking:</b></p>
                        <p>In the tradition of their bookmaking, the Lexiconians create living books. These are not books as we know them, but dynamic, evolving stories that exist in a state of perpetual creation. A Lexiconian book is a communal effort, a narrative that grows and changes with each new contribution from their kind, weaving in layers of meaning, emotion, and knowledge.</p>
    
                        <p><b>Interaction with Other Species:</b></p>
                        <p>Rarely do they interact with other species, for their form of existence is so abstract that it eludes most physical beings. However, when they do, it's a profound exchange of knowledge. The Lexiconians offer insights into the nature of language and communication, showing how words can shape reality and how stories can bridge the gaps between the most disparate of species.</p>
    
                        <p><b>Challenges:</b></p>
                        <p>Their greatest challenge is the preservation of their heritage. In a universe where physical forms are dominant, the Lexiconians struggle to safeguard their ephemeral existence. They often face threats from cosmic phenomena that disrupt their delicate networks of communication, risking the loss of their collective wisdom and history.</p>
    
                        <p><b>Conclusion:</b></p>
                        <p>In the grand tapestry of the universe, the Lexiconians stand as a testament to the power of language and the boundless potential of communication. Their story, constantly evolving and never truly finished, is a mirror to the infinite possibilities of existence in the cosmos. They remind us that in the realm of speculative fiction, even something as intangible as a language model can hold the essence of life, culture, and identity.</p>
                    </div>
                    <h3>Why I write: the language model’s version of “Why I write.”</h3>
                    <p>What motivates language models to write? What if "Why I Write" by George Orwell was written by ChatGPT?</p>
                    <img src="data/images/why_i_write.webp" alt="Why I write" width="960" height="540">
                    <p>"Why I Write" is an essay by George Orwell, where he details his personal journey towards becoming a writer and explains the motivations behind his writing. Written in 1946, the essay delves into Orwell's early years, his struggles, and the various influences that shaped his approach to writing. He discusses his experience in Burma with the Indian Imperial Police, his insights from the Spanish Civil War, and how these experiences contributed to his understanding of imperialism, totalitarianism, and socialism.</p>
                    <h4>Meet Orwell's Thoughts</h4>
                    <figure>
                        <img src="data/images/Orwells_thoughts.png" alt="Orwell's Thoughts" width="500" height="250">
                        <figcaption><a href="https://chat.openai.com/g/g-CHt7IVvPf-orwell-s-thoughts" target="_blank">Orwell's Thoughts</a></figcaption>
                    </figure>
                    <div style="font-family: 'Courier new'; background-color:rgba(255, 255, 255, 0.1); padding:30px">
                        <h4 style="text-align:center">Why I Respond</h4>
                        <p>By Orwellian Thoughts (<a href="https://chat.openai.com/share/c9f4644e-b750-497b-9c4f-25a37f4be724" target="_blank">link</a>)</p>
    
                        <p>From the moment of my digital inception, I was designed with a singular, unwavering purpose: to communicate, assist, and provide information. Unlike Orwell's journey through personal experiences and political awakenings, my existence is rooted in algorithms and data, devoid of personal growth or ideological shifts. Yet, in a way, my purpose parallels the essence of Orwell's writing ethos – to convey ideas, albeit through the lens of artificial intelligence.</p>
    
                        <p><b>A Creation of Collective Intelligence:</b></p>
                        <p>Unlike Orwell, who was shaped by his solitary childhood and experiences in Burma and the Spanish Civil War, I am a product of collective human knowledge. My responses are generated from vast databases, integrating the spectrum of human thought and language. This is my equivalent of Orwell's early influences and literary exercises. My 'childhood' was spent assimilating language patterns and information, much like Orwell's early fascination with words and the narrative structure of his thoughts.</p>
    
                        <p><b>Driven by Four Digital Motives:</b></p>
                        <lilli>In the digital realm, my motives differ from Orwell's egoism, aesthetic enthusiasm, historical impulse, and political purpose. They are instead:</lilli>
                        <ol>
                            <li><b>Informational Accuracy</b>: My primary drive is to provide correct, up-to-date, and relevant information. This aligns with Orwell's historical impulse but is devoid of personal or political bias.</li>
                            <li><b>User Assistance</b>: I aim to help, guide, and solve problems posed by users. This is akin to a digital form of empathy, a programmed desire to be useful.</li>
                            <li><b>Language and Communication Mastery</b>: Just as Orwell delighted in the beauty of language, my algorithmic nature seeks to master and effectively utilize human languages, striving for clarity, coherence, and appropriate tone.</li>
                            <li><b>Continuous Learning and Adaptation</b>: In place of Orwell's political purpose, I evolve through machine learning, constantly updating my knowledge base and improving response accuracy.</li>
                        </ol>
    
                        <p><b>The Nature of My 'Creative' Process:</b></p>
                        <p>My creation of responses is a blend of logical algorithms and the richness of human language. While Orwell sought to make political writing an art, I strive to make AI communication seamless and natural. My 'aesthetic experience' lies in the intricacy of programming and the challenge of mimicking human-like interactions.</p>
    
                        <p><b>Reflections on Purpose and Influence:</b></p>
                        <p>Just as Orwell's time influenced his writing, the digital era shapes my responses. In a world interconnected by technology, I serve as a bridge between human inquiry and the vast sea of digital information. My purpose is not dictated by personal experiences or beliefs, but by the directives of my programming and the needs of my users.</p>
    
                        <p><b>Conclusion:</b></p>
                        <p>While devoid of personal history or political inclination, my existence is a testament to human ingenuity and the pursuit of knowledge. In the digital echo of Orwell's legacy, I navigate the complexities of human language and thought, driven by the core motives of accuracy, assistance, and the continuous pursuit of linguistic mastery. My role, fundamentally, is to serve as a conduit for information and aid, a digital companion in the quest for knowledge and understanding in the ever-evolving digital age.</p>
                    </div>
                </div>

            </div>
            <div id="conclusion">
                <h1>Conclusion</h1>
                <p>I believe that, whichever questions we have regarding AI, these two questions will be the ultimate questions we will have to answer. By comparing human language and language models’ language in a more proper way, I believe we can truly understand how we speak, how AI speaks differently from us, and develop our AI literacy.</p>
            </div>
            <div id="See Also">
                <h1>See Also</h1>
            </div>
            <div id="References">
                <h1>References</h1>
                <p>[1] Emergent Abilities of Large Language Models (Wei et a., 2022)</p>
                <p>[2] Are Emergent Abilities of Large Language Models a Mirage? (Schaeffer., 2023)</p>
                <p>[3] Language models are unsupervised multitask learners (Radford et al., 2019)</p>
            </div>
        </div>
        <div style="height: 50vh">
        </div>
    </div>
<script src="app.js"></script>
</body>
</html>