<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0" />
    <meta charset="UTF-8">
    <title>LLM vs Human</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="fixed-background"></div>
    <div class="side-navigation">
        <ul>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#background">Background</a></li>
            <li><a href="#the-questions">The Questions</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
        </ul>
    </div>
    <div class="content">
        <h1 style="font-size: 50pt; color: white; text-shadow: 2px 2px 4px black; margin-top: 70vh;">Humans as Robots, Robots as Humans</h1>
        <div>
            <p class="scroll-down" style="font-size: 30px; font-family: 'Times New Roman'">Scroll down</p>
        </div>
        <div class="main-content">
            <div id="introduction">
                <h1>Introduction</h1>
                <p>The capacity for linguistic communication used to be a defining trait of human uniqueness. Yet, with language models now able to replicate this skill, we, who have seldom reflected on the essence of human identity, find ourselves deeply challenged. To be prepared for the upcoming AI era, we need to better understand the difference between the way humans and language models speak.</p>
                <p>However, the attempts to understand these two are often made on very different scales. While human languages are usually considered as emergent properties (e.g. psychology and literature), language models’ languages are analyzed with very reductionist approaches. There is no such thing as human language architecture—like the transformer model for language models—or language model psychology.</p>
            </div>
            <div id="background">
                <h1>Background</h1>
                <h2>Reductionism and Emergence</h2>
                <p>Reductionism and emergence represent contrasting, yet complementary, approaches in understanding complex systems.</p>
                <figure>
                    <img src="data/images/machine_duck.png" alt="Machine Duck" width="400" height="250">
                    <figcaption>The Digesting Duck, created by Jacques de Vaucanson, is an automaton in the form of a duck.</figcaption>
                </figure>
                <p><b>Reductionism</b> (<b>환원주의, 還元主義</b>) is a philosophical idea that a system can be fully understood by analyzing its constituent parts. This approach underpins much of classical physics and biology, where systems are deconstructed into simpler, more fundamental elements, assuming that the properties and behaviors of the whole can be fully explained by the properties and behaviors of its parts.</p>
                <p><b>Emergence</b> (<b>창발, 創發</b>), in contrast, contends that complex systems exhibit properties and behaviors that are not evident from the properties and behaviors of their individual components. These emergent properties arise from the interactions and relationships between the components, often leading to novel or unexpected phenomena that cannot be predicted solely by an understanding of the constituent parts. Emergent phenomena are prevalent in various fields, from physics (e.g., superconductivity) to biology (e.g., consciousness), and are central to complex systems and chaos theory.</p>
                <h2>Human Brain and Reductionism</h2>
                <h2>Language Models and Emergency</h2>
                <img src="data/images/emergence_graphs.webp" alt="Emergence Graphs" width="960" height="540">
                <p>Advancements in artificial intelligence have led to the creation of models that harness emergent properties. Diverging from conventional models crafted through a reductionist approach tailored to specific tasks, contemporary models adopt a scalable architecture. This scalability gives rise to emergent properties, enabling the model to deal with complex tasks intelligently. In natural language processing (NLP), these state-of-the-art models, characterized by emergent properties, have largely supplanted traditional approaches. Consequently, the reductionist methodology, once prevalent, is now considered out-dated.</p>
                <p>However, the way how engineers analyze the AI-generated text are still more of a reductionist approach. Although the text is the production of emergence, engineers attemp to analyze it in a quantitative way—measuring test accuracy mostly. In 2022, Wei et ai. published one of the first papers that explore the emerget properties of language models <a href="#emergent-abilities">[1]</a>, which is one of the milestones in the NLP filed, but this research also takes a reductionist approach to analyze emergent properties. In the following year, Schaeffer et al., proposed another way to examine the emergent properties of language models to address the inadequacy <a href="#mirage">[2]</a>, but it's still based on quantitative measurments.</p>
            </div>
            <div id="the-questions">
                <h1>The Questions</h1>
                <p>Tho questions I aim to answer in this project: 1) “What’s the transformer model’s counterpart for humans?” and 2) “Considering language models as a human, how do they speak?”</p>
                <h2>Q1. Reductionism: Human language in the perspective of AI </h2>
                <p>To understand what the transformer architecture’s counterpart is for humans, I will explore a variety of concepts ranging from biology to psychology and linguistics. For the biology part, the topics will include the regions that are responsible for language production in the brain (e.g. Wernicke’s area, Broca’s area, and auditory dorsal stream), and for the linguistics part, I will mainly talk about universal grammar.</p>
                <ul>
                    <li>Listening & Reading</li>
                    <li>Speaking & Writing</li>
                </ul>
                <h2>Q2. Emergence: AI-generated text in the perspective of humans</h2>
                <p>The emergent properties that language models have will be explored by projecting them onto the stories and essays from the HSS319 course. While scientists attempt to explain phenomena from the fundamental principles, authors do the opposite of what scientists do: describing phenomena on a more abstract scale. With the help of the authors, we can project the language models onto their creations where we can observe the production of language models on new dimensions.</p>
                <h3>Story of Your Life: Language model in the perspective of Heptapods</h3>
                <iframe width="960" height="540" src="https://www.youtube.com/embed/AZ4oGBgxiuY?si=NCPb7hzDuvW-cGzE&amp;start=90" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                <p>Imagine you are playing chess. In the beginning, you don’t know how this game will end because you can’t see all the possible cases ahead. But, with the several moves you can see ahead, you play chess in a quite intelligent way.</p>
                <p>Writing is similar to playing chess. Although you don’t know how it will end, you can see some words ahead and that allows you to write in a quite intelligent way. If you were a Heptapod, you would have been able to see all the possible movements and write in the best way possible. What about ChatGPT? How many moves can it see ahead? Only one.</p>
                <p>This is called beam search in computer science. It’s an algorithm whereby ChatGPT finds the next word, and it doesn’t let you see what set of words you will get after the next turn. It’s playing chess without seeing any moves ahead of the one it is now making.</p>
                <h3>The Bookmaking Habits of Select Species: what if language models were another species?</h3>
                <img src="data/images/bookmaking_habit.png" alt="The Bookmaking Habits of Select Species" width="960" height="540">
                <p>What if language models were another species? What if they were a species that could only communicate through books? What would their books look like? What would their books be about?</p>
                <h3>Why I write: the language model’s version of “Why I write.”</h3>
                <p>What motivates language models to write? What if "Why I Write" by George Orwell was written by ChatGPT?</p>
                <img src="data/images/why_i_write.webp" alt="Why I write" width="960" height="540">
            </div>
            <div id="conclusion">
                <h1>Conclusion</h1>
                <p>I believe that, whichever questions we have regarding AI, these two questions will be the ultimate questions we will have to answer. By comparing human language and language models’ language in a more proper way, I believe we can truly understand how we speak, how AI speaks differently from us, and develop our AI literacy.</p>
            </div>
            <div id="See Also">
                <h1>See Also</h1>
            </div>
            <div id="References">
                <h1>References</h1>
                <p>[1] <a id="emergent-abilities" href="https://arxiv.org/pdf/2206.07682.pdf">Emergent Abilities of Large Language Models (Wei et a., 2022)</a></p>
                <p>[2] <a id="mirage" href="https://arxiv.org/pdf/2304.15004.pdf">Are Emergent Abilities of Large Language Models a Mirage? (Schaeffer., 2023)</a></p>
            </div>
        </div>
        <div style="height: 50vh">
        </div>
    </div>
<script src="app.js"></script>
</body>
</html>